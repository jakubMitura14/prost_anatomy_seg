{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import monai\n",
    "import os\n",
    "from os.path import basename, dirname, exists, isdir, join, split\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from toolz.itertoolz import groupby\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "\n",
    "root_dir='/workspaces/prost_anatomy_seg/data/550'\n",
    "\n",
    "def analyze_single_label(uniq_num,centers, big_mask, connected,in_min):\n",
    "    infered_inner=(connected==uniq_num)\n",
    "    total=np.sum(infered_inner.flatten())\n",
    "    inn= np.sum(np.logical_and(infered_inner,big_mask).flatten())/total\n",
    "    # cov= np.sum(np.logical_and(infered,centers).flatten())/np.sum(centers.flatten())\n",
    "    res= (inn>in_min) #and (cov>cover_min)\n",
    "    return res\n",
    "\n",
    "def get_my_specifity(bi,inn,twos,curr,epoch,folder_path,batch_id,bigger_mask):\n",
    "    centers= twos[bi,:,:,:]\n",
    "    inferred=curr[bi,:,:,:]\n",
    "    big_mask=bigger_mask[bi,:,:,:]\n",
    "\n",
    "    if(np.sum(inferred.flatten())==0):\n",
    "        return 1.0\n",
    "\n",
    "    connected=sitk.GetArrayFromImage(sitk.ConnectedComponent(sitk.GetImageFromArray(inferred.astype(int))))\n",
    "    uniqq=np.unique(connected)\n",
    "    uniqq= list(filter(lambda el:el>0,uniqq))\n",
    "    in_min=0.5\n",
    "    res= list(map(lambda uniq_num: analyze_single_label(uniq_num,centers, big_mask, connected,in_min), uniqq))\n",
    "    res= np.mean(np.array(res).astype(int))\n",
    "    return res\n",
    "\n",
    "def is_sth_in_areas(uniq_num,arr,inferred):\n",
    "\n",
    "    bool_arr=(arr.copy()==uniq_num)\n",
    "    summ=np.sum(inferred[bool_arr].flatten())\n",
    "    res= summ>0\n",
    "    return res\n",
    "\n",
    "def get_connected_components_num(arr):\n",
    "    connected=sitk.GetArrayFromImage(sitk.ConnectedComponent(sitk.GetImageFromArray(arr.astype(int))))\n",
    "    uniqq=np.unique(connected)\n",
    "    uniqq= list(filter(lambda el:el>0,uniqq))\n",
    "    return len(uniqq)    \n",
    "\n",
    "def get_my_sensitivity(bi,inn,twos,curr,epoch,folder_path,batch_id,bigger_mask):\n",
    "    curr_in= inn[bi,:,:,:]\n",
    "    curr_twos= twos[bi,:,:,:]\n",
    "    inferred=curr[bi,:,:,:]\n",
    "    curr_bigger_mask=bigger_mask[bi,:,:,:]\n",
    "    if(epoch%10==0):\n",
    "        curr_num=bi*100+batch_id\n",
    "        # sitk.WriteImage(sitk.GetImageFromArray(curr_curr.astype(np.uint8)), f\"{folder_path}/{curr_num}_inferred.nii.gz\")\n",
    "        # sitk.WriteImage(sitk.GetImageFromArray(curr_bigger_mask.astype(np.uint8)), f\"{folder_path}/{curr_num}_big_mask.nii.gz\")\n",
    "        # sitk.WriteImage(sitk.GetImageFromArray(curr_twos.astype(np.uint8)), f\"{folder_path}/{curr_num}_centers.nii.gz\")\n",
    "        \n",
    "    \n",
    "    total = np.sum(curr_in.flatten())\n",
    "    total_twos = np.sum(curr_twos.flatten())\n",
    "    curr_percent_in=np.zeros(1)\n",
    "    curr_percent_covered=np.zeros(1)\n",
    "\n",
    "    if(total_twos==0):\n",
    "        return -1.0\n",
    "    if(total==0):\n",
    "        return 0.0\n",
    "    \n",
    "    components_ones = get_connected_components_num(curr_bigger_mask)\n",
    "    components_twos = get_connected_components_num(curr_twos)\n",
    "\n",
    "    arr=curr_bigger_mask\n",
    "    if(components_twos>components_ones):\n",
    "        print(f\"ones are mergeddd\")\n",
    "        arr=curr_twos\n",
    "\n",
    "    connected=sitk.GetArrayFromImage(sitk.ConnectedComponent(sitk.GetImageFromArray(arr.astype(int))))\n",
    "    uniqq=np.unique(connected)\n",
    "    uniqq= list(filter(lambda el:el>0,uniqq))\n",
    "\n",
    "\n",
    "    res=np.array(list(map(lambda uniq_num: is_sth_in_areas(uniq_num,connected,inferred),uniqq)))\n",
    "    res=res.astype(int)\n",
    "    return np.mean(res.flatten())\n",
    "    # curr_percent_in=np.sum(curr_in.flatten())/(total)\n",
    "    # curr_percent_covered=  np.sum(((curr_in) & (curr_twos)).flatten())/ total_twos\n",
    "\n",
    "    # return (curr_percent_in>0.8 and curr_percent_covered>0.4).astype(float)\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_files(root_dir):\n",
    "    \"\"\" \n",
    "    based on name will group the files into triplets of related files\n",
    "    \"\"\"\n",
    "    all_files=os.listdir(root_dir)\n",
    "    grouped_by_master= groupby(lambda path : path.split('_')[0],all_files)\n",
    "    return list(dict(grouped_by_master).items())\n",
    "\n",
    "grouped=group_files(root_dir)    \n",
    "def chunks(xs, n):\n",
    "    n = max(1, n)\n",
    "    return (xs[i:i+n] for i in range(0, len(xs), n))\n",
    "# aa=list(map(lambda g:analyze_case(g,root_dir)  ,grouped))\n",
    "grouped=list(chunks(grouped,10))\n",
    "grouped=sorted(grouped, key=lambda el: el[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_in= np.zeros(1)\n",
    "# percent_out=np.zeros(1)\n",
    "# percent_covered=np.zeros(1)\n",
    "# is_correct=np.zeros(1)\n",
    "# my_sensitivity=np.zeros(1)\n",
    "# my_specificity=np.zeros(1)\n",
    "\n",
    "grouped\n",
    "\n",
    "def getarrs_files(group):\n",
    "    num,paths=group\n",
    "    paths= list(map(lambda p: f\"{root_dir}/{p}\",paths))\n",
    "    centers=sitk.GetArrayFromImage(sitk.ReadImage(list(filter(lambda pathh: 'centers' in pathh  ,paths))[0])).astype(bool)\n",
    "    big_mask=sitk.GetArrayFromImage(sitk.ReadImage(list(filter(lambda pathh: 'big_mask' in pathh  ,paths))[0])).astype(bool)\n",
    "    inferred=sitk.ReadImage(list(filter(lambda pathh: 'inferred' in pathh  ,paths))[0])\n",
    "    inferred=sitk.GetArrayFromImage(inferred)\n",
    "    return (centers,big_mask,inferred)\n",
    "\n",
    "def analyze_cases(group,root_dir):\n",
    "    percent_in= np.zeros(1)\n",
    "    percent_out=np.zeros(1)\n",
    "    percent_covered=np.zeros(1)\n",
    "    is_correct=np.zeros(1)\n",
    "    my_sensitivity=np.zeros(1)\n",
    "    my_specificity=np.zeros(1)\n",
    "\n",
    "    epoch=5\n",
    "    batch_id=1\n",
    "\n",
    "\n",
    "    arrs=list(map(getarrs_files, group))\n",
    "\n",
    "    centers= np.stack(list(map(lambda el: el[0],arrs)))\n",
    "    big_mask= np.stack(list(map(lambda el: el[1],arrs)))\n",
    "    inferred= np.stack(list(map(lambda el: el[2],arrs)))\n",
    "\n",
    "    shapp=inferred.shape\n",
    "\n",
    "    bigger_mask= big_mask\n",
    "    curr=inferred\n",
    "    \n",
    "    # curr= torch.sum(curr,dim=1)\n",
    "    inn = curr & bigger_mask\n",
    "    twos= centers\n",
    "\n",
    "\n",
    "    # curr= np.round(np.random.random(curr.shape)).astype(bool) #TODO remove            \n",
    "\n",
    "        \n",
    "    \n",
    "    base='/workspaces/konwersjaJsonData/explore/validation_to_look_into'\n",
    "    folder_path=f\"{base}/{epoch}\"\n",
    "    os.makedirs(folder_path,exist_ok=True)\n",
    "    \n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "       my_sensitivity=pool.map(partial(get_my_sensitivity,inn=inn,twos=twos,curr=curr,epoch=epoch,folder_path=folder_path,batch_id=batch_id,bigger_mask=bigger_mask),range(shapp[0]))\n",
    "    \n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        my_specificity=pool.map(partial(get_my_specifity,inn=inn,twos=twos,curr=curr,epoch=epoch,folder_path=folder_path,batch_id=batch_id,bigger_mask=bigger_mask),range(shapp[0]))\n",
    "    \n",
    "\n",
    "    # print(f\"rrrrrrr {res}\")\n",
    "    my_sensitivity=list(filter(lambda el: np.array(el).flatten()[0]>-1,my_sensitivity  ))      \n",
    "\n",
    "    my_sensitivity= np.array(list(map(lambda el: np.mean(np.array(el).flatten()) ,my_sensitivity)))\n",
    "    my_specificity= np.array(list(map(lambda el: np.mean(np.array(el).flatten()) ,my_specificity)))\n",
    "\n",
    "    # if(len(my_sensitivity)>0):\n",
    "    #     my_sensitivity= np.mean(np.array(list(map(lambda el : np.array(np.mean(el)).flatten(),my_sensitivity))))\n",
    "    #     my_specificity= np.mean(np.array(list(map(lambda el : np.array(np.mean(el)).flatten(),my_specificity))))\n",
    "    #     is_correct= (my_sensitivity+my_specificity)/2\n",
    "    \n",
    "    # total = np.sum(curr.flatten())\n",
    "    \n",
    "    # if(total>0):\n",
    "    #     percent_in=np.sum(inn.flatten())/(total)\n",
    "    # if(total>0):\n",
    "    #     out = (curr) & (~bigger_mask)\n",
    "    #     percent_out= np.sum(out.flatten())/total\n",
    "\n",
    "    \n",
    "    # two_sum=np.sum(twos.flatten())\n",
    "    # if(two_sum>0):\n",
    "    #     percent_covered=  np.sum(((curr) & (twos)).flatten())/two_sum\n",
    "    \n",
    "    # # print(f\"tttttt is_correct {is_correct} total {total} ((curr) & (twos)).sum() {((curr) & (twos)).sum()}  (curr) & (~bigger_mask) {((curr) & (~bigger_mask)).sum()}\")\n",
    "    # is_correct=np.array(is_correct).flatten()\n",
    "\n",
    "    my_sensitivity=np.array([np.mean(my_sensitivity)])\n",
    "    my_specificity=np.array([np.mean(my_specificity)])\n",
    "\n",
    "    # percent_in=np.array([percent_in]).flatten()\n",
    "    # percent_out=np.array([percent_out]).flatten()\n",
    "    # percent_covered=np.array([percent_covered]).flatten()\n",
    "\n",
    "    print(f\"my_sensitivity {my_sensitivity} my_specificity {my_specificity}\")\n",
    "\n",
    "    return (my_sensitivity,my_specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones are mergeddd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_sensitivity [0.33333333] my_specificity [0.7]\n",
      "my_sensitivity [0.75] my_specificity [1.]\n",
      "my_sensitivity [0.6] my_specificity [1.]\n",
      "my_sensitivity [0.75] my_specificity [0.85]\n",
      "ones are mergeddd\n",
      "my_sensitivity [0.58333333] my_specificity [0.9]\n",
      "my_sensitivity [0.375] my_specificity [0.8]\n",
      "my_sensitivity [0.6] my_specificity [0.9]\n",
      "my_sensitivity [0.6] my_specificity [1.]\n",
      "my_sensitivity [0.75] my_specificity [0.8]\n",
      "ones are mergeddd\n",
      "my_sensitivity [0.73809524] my_specificity [0.9]\n",
      "my_sensitivity [0.8] my_specificity [1.]\n",
      "my_sensitivity [0.72222222] my_specificity [0.75]\n",
      "my_sensitivity [0.91666667] my_specificity [0.95]\n",
      "my_sensitivity [0.6] my_specificity [0.9]\n",
      "ones are mergeddd\n",
      "ones are mergeddd\n",
      "my_sensitivity [0.71428571] my_specificity [0.9]\n",
      "my_sensitivity [0.83333333] my_specificity [0.85]\n",
      "my_sensitivity [0.6] my_specificity [0.7]\n",
      "my_sensitivity [0.875] my_specificity [0.95]\n",
      "my_sensitivity [0.66666667] my_specificity [0.8]\n",
      "my_sensitivity [0.83333333] my_specificity [0.9]\n",
      "my_sensitivity [0.36666667] my_specificity [0.8]\n",
      "my_sensitivity [0.64285714] my_specificity [0.9]\n",
      "my_sensitivity [0.83333333] my_specificity [0.9]\n",
      "ones are mergeddd\n",
      "my_sensitivity [0.41666667] my_specificity [1.]\n",
      "my_sensitivity [0.55555556] my_specificity [1.]\n",
      "my_sensitivity [0.75] my_specificity [1.]\n",
      "my_sensitivity [1.] my_specificity [0.95]\n",
      "my_sensitivity [0.5] my_specificity [0.7]\n",
      "my_sensitivity [0.71428571] my_specificity [0.8]\n",
      "my_sensitivity [0.83333333] my_specificity [0.93333333]\n",
      "my_sensitivity [0.6] my_specificity [0.9]\n",
      "my_sensitivity [0.8] my_specificity [0.8]\n",
      "my_sensitivity [0.91666667] my_specificity [1.]\n",
      "my_sensitivity [0.35714286] my_specificity [0.9]\n",
      "my_sensitivity [0.72222222] my_specificity [0.8]\n",
      "my_sensitivity [0.66666667] my_specificity [0.9]\n",
      "my_sensitivity [0.66666667] my_specificity [1.]\n",
      "my_sensitivity [0.77083333] my_specificity [0.9]\n",
      "my_sensitivity [0.64583333] my_specificity [0.9]\n",
      "my_sensitivity [1.] my_specificity [0.9]\n",
      "my_sensitivity [0.78571429] my_specificity [0.75]\n",
      "my_sensitivity [0.7] my_specificity [0.86666667]\n",
      "my_sensitivity [0.71428571] my_specificity [0.85]\n",
      "my_sensitivity [0.5] my_specificity [0.85]\n",
      "my_sensitivity [0.8] my_specificity [1.]\n",
      "my_sensitivity [0.33333333] my_specificity [1.]\n",
      "my_sensitivity [0.5] my_specificity [0.8]\n",
      "my_sensitivity [0.71428571] my_specificity [1.]\n",
      "ones are mergeddd\n",
      "my_sensitivity [0.38888889] my_specificity [0.9]\n",
      "my_sensitivity [0.33333333] my_specificity [0.6]\n",
      "my_sensitivity [0.71428571] my_specificity [0.8]\n",
      "my_sensitivity [0.5] my_specificity [0.8]\n",
      "my_sensitivity [0.92857143] my_specificity [0.95]\n",
      "my_sensitivity [0.66666667] my_specificity [0.75]\n",
      "my_sensitivity [0.66666667] my_specificity [1.]\n",
      "my_sensitivity [0.57142857] my_specificity [0.9]\n",
      "my_sensitivity [0.38095238] my_specificity [1.]\n",
      "my_sensitivity [0.57142857] my_specificity [0.8]\n",
      "my_sensitivity [0.9] my_specificity [0.9]\n",
      "my_sensitivity [0.6875] my_specificity [0.85]\n",
      "my_sensitivity [0.83333333] my_specificity [0.9]\n",
      "my_sensitivity [0.61111111] my_specificity [1.]\n",
      "my_sensitivity [0.66666667] my_specificity [1.]\n",
      "my_sensitivity [0.7] my_specificity [0.9]\n",
      "my_sensitivity [0.5] my_specificity [0.7]\n",
      "my_sensitivity [1.] my_specificity [0.8]\n",
      "my_sensitivity [0.85714286] my_specificity [1.]\n",
      "my_sensitivity [0.76190476] my_specificity [0.9]\n",
      "my_sensitivity [0.71428571] my_specificity [0.9]\n",
      "my_sensitivity [0.8] my_specificity [1.]\n",
      "my_sensitivity [0.66666667] my_specificity [1.]\n",
      "my_sensitivity [0.66666667] my_specificity [1.]\n",
      "my_sensitivity [0.4] my_specificity [0.9]\n",
      "my_sensitivity [0.66666667] my_specificity [1.]\n",
      "my_sensitivity [1.] my_specificity [0.85]\n",
      "my_sensitivity [0.375] my_specificity [0.9]\n",
      "my_sensitivity [0.7] my_specificity [0.85]\n",
      "ones are mergeddd\n",
      "my_sensitivity [0.65] my_specificity [0.95]\n",
      "my_sensitivity [0.57142857] my_specificity [0.8]\n",
      "my_sensitivity [0.75] my_specificity [1.]\n",
      "my_sensitivity [0.8] my_specificity [1.]\n",
      "my_sensitivity [0.54166667] my_specificity [1.]\n",
      "my_sensitivity [0.58333333] my_specificity [0.9]\n",
      "my_sensitivity [0.78571429] my_specificity [0.9]\n",
      "my_sensitivity [0.75] my_specificity [0.8]\n",
      "ones are mergeddd\n",
      "my_sensitivity [0.61904762] my_specificity [1.]\n",
      "my_sensitivity [0.75] my_specificity [1.]\n",
      "my_sensitivity [0.66666667] my_specificity [0.8]\n",
      "my_sensitivity [0.66666667] my_specificity [0.8]\n",
      "my_sensitivity [1.] my_specificity [0.95]\n",
      "my_sensitivity [0.68518519] my_specificity [0.93333333]\n",
      "my_sensitivity [0.5] my_specificity [0.95]\n",
      "my_sensitivity [0.94444444] my_specificity [1.]\n",
      "my_sensitivity [0.75] my_specificity [1.]\n",
      "my_sensitivity [0.75] my_specificity [0.86666667]\n"
     ]
    }
   ],
   "source": [
    "rrrr= list(map(lambda group: analyze_cases(group,root_dir),grouped))\n",
    "# #num 808\n",
    "# # my_sensitivity [0.33333333] my_specificity [0.5]\n",
    "# group=list(filter(lambda el: int(el[0][0])==808,grouped))[0]\n",
    "# analyze_cases(group,root_dir)\n",
    "\n",
    "# # grouped[100][0]\n",
    "# group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sensitivity 0.6783256752993594 specificity 0.8963157894736843\n"
     ]
    }
   ],
   "source": [
    "a = list(map(lambda el: el[0],rrrr))\n",
    "b = list(map(lambda el: el[1],rrrr))\n",
    "print(f\" sensitivity {np.mean(np.array(a).flatten())} specificity {np.mean(np.array(b).flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 48, 56)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "arr=sitk.ReadImage(f\"{root_dir}/808_big_mask.nii.gz\")\n",
    "inferred=sitk.GetArrayFromImage(sitk.ReadImage(f\"{root_dir}/808_inferred.nii.gz\"))\n",
    "connected=sitk.GetArrayFromImage(sitk.ConnectedComponent(arr))\n",
    "booll= (connected==3 )\n",
    "\n",
    "np.sum(inferred[booll].flatten())\n",
    "connected.shape\n",
    "# sitk.WriteImage(sitk.ConnectedComponent(arr), '/workspaces/prost_anatomy_seg/data/test.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6783256752993594"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tttt=list(map(lambda el: el[0] ,rrrr))\n",
    "tttt= list(filter(lambda el: el>-1,tttt))\n",
    "np.nanmean(tttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39mmean(aa)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aa' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(aa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_is_correct(curr_in,curr_twos):\n",
    "    total = curr_in.sum()\n",
    "    curr_percent_in=np.zeros(1)\n",
    "    curr_percent_covered=np.zeros(1)\n",
    "\n",
    "    if(total.item()>0):\n",
    "        curr_percent_in=curr_in.sum()/(total)\n",
    "    if(curr_twos.sum().item()>0):\n",
    "        curr_percent_covered=  ((curr_in) & (curr_twos)).sum()/ curr_twos.sum()\n",
    "\n",
    "    return (curr_percent_in>0.8 and curr_percent_covered>0.4)\n",
    "\n",
    "\n",
    "def analyze_case_sum(group,root_dir):\n",
    "    num,paths=group\n",
    "    paths= list(map(lambda p: f\"{root_dir}/{p}\",paths))\n",
    "\n",
    "    centers=sitk.GetArrayFromImage(sitk.ReadImage(list(filter(lambda pathh: 'centers' in pathh  ,paths))[0])).astype(bool)\n",
    "    big_mask=sitk.GetArrayFromImage(sitk.ReadImage(list(filter(lambda pathh: 'big_mask' in pathh  ,paths))[0])).astype(bool)\n",
    "    inferred=sitk.ReadImage(list(filter(lambda pathh: 'inferred' in pathh  ,paths))[0])\n",
    "    connected=sitk.GetArrayFromImage(inferred)\n",
    "    connected=(connected>0)\n",
    "    # uniqq= list(filter(lambda el:el>0,uniqq))\n",
    "    cover_min=0.5\n",
    "    in_min=0.8\n",
    "    \n",
    "    inn = connected & big_mask\n",
    "\n",
    "    res=get_is_correct(inn,centers)\n",
    "    # res= analyze_single_label(True,centers, big_mask, connected,cover_min,in_min)\n",
    "    res= np.mean(np.array(res).astype(int))\n",
    "    return (num,res)\n",
    "\n",
    "grouped=group_files(root_dir)\n",
    "\n",
    "bb=list(map(lambda g:analyze_case_sum(g,root_dir)  ,grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(bb)\n",
    "bbb= list(map(lambda tupl: tupl[1],bb))\n",
    "# np.mean(bbb)\n",
    "np.mean(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
